# üöÄ Am√©liorations Avanc√©es de l'Import Automatique

## R√©sum√© des Nouvelles Fonctionnalit√©s

Cette mise √† jour majeure ajoute 5 fonctionnalit√©s avanc√©es au syst√®me d'import automatique de biens immobiliers :

1. ‚úÖ **Suivi des redirections JavaScript** - Rendu des pages dynamiques avec Ferrum
2. ‚úÖ **G√©ocoding automatique** - Conversion adresse ‚Üí coordonn√©es GPS
3. ‚úÖ **Extraction automatique d'images** - T√©l√©chargement des photos d'annonces
4. ‚úÖ **Syst√®me de cache intelligent** - √âvite les re-scraping inutiles
5. ‚úÖ **Support √©tendu multi-plateformes** - 5 nouvelles plateformes support√©es

---

## 1. üîÑ Suivi des Redirections JavaScript

### Fonctionnalit√©

Certains sites utilisent du JavaScript pour charger le contenu dynamiquement. Le nouveau service `JavascriptRendererService` utilise **Ferrum** (Chrome headless) pour rendre les pages JavaScript.

### Utilisation

```ruby
# Activer le rendu JavaScript
scraper = PropertyScraperService.new(url, javascript: true)
data = scraper.call
```

### Configuration

Le rendu JavaScript est d√©sactiv√© par d√©faut pour des raisons de performance. Il sera automatiquement utilis√© en fallback si le scraping basique √©choue.

**D√©pendances** : Ferrum n√©cessite Chrome/Chromium install√© sur le syst√®me.

---

## 2. üó∫Ô∏è G√©ocoding Automatique

### Fonctionnalit√©

Convertit automatiquement **ville + code postal** en coordonn√©es GPS (latitude/longitude) via l'API Nominatim d'OpenStreetMap.

### Service : `GeocodingService`

```ruby
service = GeocodingService.new("Paris", "75001", "10 rue de Rivoli")
result = service.call
# => { latitude: 48.8566, longitude: 2.3522 }
```

### Int√©gration

Le g√©ocoding est **automatique** lors de l'import. Les coordonn√©es sont directement ajout√©es aux donn√©es extraites :

```ruby
scraper = PropertyScraperService.new(url)
data = scraper.call
# data contient maintenant :latitude et :longitude
```

### Configuration

Fichier : `config/initializers/geocoder.rb`

```ruby
Geocoder.configure(
  lookup: :nominatim,  # Provider gratuit OpenStreetMap
  timeout: 5,
  cache: Rails.cache,  # Cache les r√©sultats
  nominatim: {
    host: "nominatim.openstreetmap.org",
    email: "contact@achat-immo.fr"
  }
)
```

**Limites** : Nominatim a un rate limit de 1 requ√™te/seconde. Le cache √©vite les requ√™tes r√©p√©t√©es.

### D√©sactiver le g√©ocoding

```ruby
scraper = PropertyScraperService.new(url, geocode: false)
```

---

## 3. üì∏ Extraction Automatique d'Images

### Fonctionnalit√©

Extrait automatiquement les URLs des photos d'annonces depuis :
- JSON-LD (schema.org)
- Meta tags Open Graph
- Balises `<img>` avec classes sp√©cifiques

### Service : `PropertyImageExtractorService`

Le service extrait et filtre les images pertinentes (max 10 par d√©faut).

### Utilisation Automatique

```ruby
scraper = PropertyScraperService.new(url)
data = scraper.call

# Les URLs d'images sont disponibles
puts scraper.image_urls
# => ["https://example.com/photo1.jpg", "https://example.com/photo2.jpg"]
```

### T√©l√©chargement et Attachement

Les images peuvent √™tre automatiquement t√©l√©charg√©es et attach√©es √† un bien :

```ruby
scraper = PropertyScraperService.new(url)
data = scraper.call

property = Property.create!(data)
scraper.extract_and_attach_images(property)
# Les photos sont maintenant attach√©es via Active Storage
```

### Filtres

Le service ignore automatiquement :
- SVG et GIF (ic√¥nes)
- Images contenant "logo", "icon", "placeholder"
- Data URIs
- URLs trop longues (> 2000 caract√®res)

### Configuration

```ruby
# D√©sactiver l'extraction d'images
scraper = PropertyScraperService.new(url, images: false)
```

---

## 4. üíæ Syst√®me de Cache Intelligent

### Fonctionnalit√©

√âvite de re-scraper la m√™me URL plusieurs fois. Les r√©sultats sont mis en cache pendant **7 jours**.

### Mod√®le : `PropertyScrapeCache`

```ruby
# Structure de la table
create_table :property_scrape_caches do |t|
  t.string :url_hash              # SHA256 de l'URL
  t.jsonb :scraped_data           # Donn√©es extraites
  t.jsonb :images_urls            # URLs des images
  t.datetime :expires_at          # Date d'expiration
  t.timestamps
end
```

### Utilisation Automatique

Le cache est **automatiquement v√©rifi√©** lors de chaque scraping :

```ruby
scraper = PropertyScraperService.new(url)
data = scraper.call  # V√©rifie le cache en premier
```

**Logs** :
```
PropertyScraperService: Cache hit for https://example.com/annonce/123
```

### Gestion Manuelle

```ruby
# V√©rifier si une URL est en cache
cache = PropertyScrapeCache.find_by_url(url)

# Cr√©er/mettre √† jour un cache
PropertyScrapeCache.cache_for_url(url, data, image_urls)

# Nettoyer les caches expir√©s
PropertyScrapeCache.cleanup_expired!
```

### D√©sactiver le Cache

```ruby
scraper = PropertyScraperService.new(url, cache: false)
```

### Nettoyage Automatique

Un job r√©current nettoie les caches expir√©s chaque jour √† 3h :

```yaml
# config/recurring.yml
clean_expired_scrape_caches:
  class: CleanExpiredScrapeCachesJob
  schedule: every day at 3am
```

---

## 5. üåê Support Multi-Plateformes √âtendu

### Nouvelles Plateformes Support√©es

| Plateforme | URL Pattern | Extracteur |
|------------|-------------|------------|
| **Logic-immo** | `logic-immo.com` | `extract_from_logic_immo` |
| **Orpi** | `orpi.com` | `extract_from_orpi` |
| **Century21** | `century21.fr` | `extract_from_century21` |
| **Lafor√™t** | `laforet.com` | `extract_from_laforet` |
| **Figaro Immobilier** | `proprietes.lefigaro.fr` | `extract_from_figaro_immo` |

### Plateformes Existantes (d√©j√† support√©es)

- ‚úÖ Jinka (redirections)
- ‚úÖ SeLoger
- ‚úÖ LeBonCoin
- ‚úÖ PAP
- ‚úÖ Bien'ici

### Extraction G√©n√©rique

Si le site n'est pas reconnu, l'extracteur g√©n√©rique tente d'extraire les donn√©es via :
- JSON-LD (schema.org)
- Meta tags Open Graph
- Patterns HTML g√©n√©riques

### Ajout d'une Nouvelle Plateforme

1. Ajouter le pattern dans les constantes :
```ruby
NOUVEAU_SITE_PATTERN = %r{nouveausite\.com}
```

2. Ajouter dans le `case` statement :
```ruby
when NOUVEAU_SITE_PATTERN
  extract_from_nouveau_site(resolved_url)
```

3. Cr√©er la m√©thode d'extraction :
```ruby
def extract_from_nouveau_site(url)
  html = fetch_html(url)
  return nil unless html
  
  # Logique d'extraction sp√©cifique...
end
```

---

## üìä API Response Enrichie

L'endpoint `/properties/import_from_url` retourne maintenant plus d'informations :

```json
{
  "success": true,
  "data": {
    "title": "Appartement T3 75m¬≤",
    "price": 350000,
    "surface": 75.0,
    "rooms": 3,
    "bedrooms": 2,
    "city": "Paris",
    "postal_code": "75015",
    "latitude": 48.8420,
    "longitude": 2.2920,
    "property_type": "appartement",
    "energy_class": "C",
    "ges_class": "D",
    "listing_url": "https://..."
  },
  "image_urls": [
    "https://cdn.example.com/photo1.jpg",
    "https://cdn.example.com/photo2.jpg"
  ],
  "images_count": 2
}
```

---

## üé® Interface Utilisateur Am√©lior√©e

Le message de succ√®s affiche maintenant le nombre de photos trouv√©es :

```
‚úÖ üì∏ Donn√©es import√©es avec succ√®s ! (5 photos trouv√©es)
```

---

## ‚öôÔ∏è Options de Configuration

Le `PropertyScraperService` accepte maintenant des options :

```ruby
PropertyScraperService.new(url, {
  cache: true,        # Utiliser le cache (d√©faut: true)
  images: true,       # Extraire les images (d√©faut: true)
  geocode: true,      # G√©ocoder l'adresse (d√©faut: true)
  javascript: false   # Utiliser Ferrum (d√©faut: false)
})
```

---

## üîß Installation et Configuration

### 1. Gems Install√©es

```ruby
# Gemfile
gem "geocoder"      # G√©ocoding
gem "down", "~> 5.0"  # T√©l√©chargement d'images
gem "ferrum"        # Browser automation
```

### 2. Migration

```bash
bin/rails db:migrate
# Cr√©e la table property_scrape_caches
```

### 3. Configuration Geocoder

Fichier cr√©√© : `config/initializers/geocoder.rb`

### 4. Job R√©current

Configur√© dans `config/recurring.yml` pour nettoyer les caches expir√©s.

---

## üìà Performance et Limites

### Cache
- **Dur√©e** : 7 jours (configurable via `PropertyScrapeCache::CACHE_DURATION`)
- **Stockage** : JSONB PostgreSQL
- **Nettoyage** : Automatique chaque jour √† 3h

### G√©ocoding
- **Provider** : Nominatim (OpenStreetMap)
- **Rate Limit** : 1 requ√™te/seconde
- **Cache** : Redis/Solid Cache
- **Fallback** : Si √©chec, les coordonn√©es restent vides

### Images
- **Maximum** : 10 images par annonce (configurable)
- **Taille max** : 10 Mo par image
- **Formats** : JPG, PNG (SVG/GIF ignor√©s)
- **Stockage** : Active Storage

### JavaScript Rendering
- **Engine** : Ferrum (Chrome headless)
- **Performance** : ~2-5 secondes par page
- **Utilisation** : D√©sactiv√© par d√©faut
- **D√©pendances** : Chrome/Chromium requis

---

## üß™ Tests

### Test Complet

```ruby
# Dans la console Rails
url = "https://www.seloger.com/annonces/achat/..."

scraper = PropertyScraperService.new(url)
data = scraper.call

puts "‚úÖ Donn√©es extraites :"
pp data

puts "\nüì∏ Images trouv√©es : #{scraper.image_urls.size}"
scraper.image_urls.each_with_index do |img_url, i|
  puts "  #{i + 1}. #{img_url}"
end

puts "\n‚ùå Erreurs :" if scraper.errors.any?
pp scraper.errors
```

### Test du Cache

```ruby
# Premi√®re extraction (pas de cache)
scraper1 = PropertyScraperService.new(url)
data1 = scraper1.call

# Deuxi√®me extraction (utilise le cache)
scraper2 = PropertyScraperService.new(url)
data2 = scraper2.call

# V√©rifier le cache
cache = PropertyScrapeCache.find_by_url(url)
puts "Cache expires at: #{cache.expires_at}"
```

### Test du G√©ocoding

```ruby
service = GeocodingService.new("Lyon", "69001")
coords = service.call

puts "Latitude: #{coords[:latitude]}"
puts "Longitude: #{coords[:longitude]}"
```

### Test des Images

```ruby
scraper = PropertyScraperService.new(url)
data = scraper.call

property = Property.create!(household: household, **data)
scraper.extract_and_attach_images(property)

puts "Photos attach√©es : #{property.photos.count}"
```

---

## üêõ D√©bogage

### Logs D√©taill√©s

```bash
# Suivre les logs en temps r√©el
tail -f log/development.log | grep "PropertyScraperService\|GeocodingService\|PropertyImageExtractorService"
```

### V√©rifier le Cache

```ruby
# Voir tous les caches actifs
PropertyScrapeCache.active.each do |cache|
  puts "URL hash: #{cache.url_hash}"
  puts "Expires: #{cache.expires_at}"
  puts "Images: #{cache.images_urls&.size || 0}"
end
```

### Nettoyer le Cache Manuellement

```ruby
# Supprimer tous les caches
PropertyScrapeCache.destroy_all

# Supprimer les caches expir√©s
PropertyScrapeCache.cleanup_expired!
```

---

## üìù Notes Techniques

### Architecture

```
PropertyScraperService (orchestrateur principal)
‚îú‚îÄ‚îÄ PropertyScrapeCache (gestion du cache)
‚îú‚îÄ‚îÄ JavascriptRendererService (rendu JavaScript optionnel)
‚îú‚îÄ‚îÄ GeocodingService (conversion adresse ‚Üí GPS)
‚îî‚îÄ‚îÄ PropertyImageExtractorService (extraction images)
```

### Flux d'Ex√©cution

1. V√©rification du cache
2. R√©solution des redirections (Jinka)
3. D√©tection de la plateforme
4. Extraction des donn√©es (HTTP ou JavaScript)
5. Extraction des images
6. G√©ocoding de l'adresse
7. Mise en cache des r√©sultats

### S√©curit√©

- Rate limiting recommand√© au niveau contr√¥leur
- Validation des URLs
- Taille maximale des images (10 Mo)
- Timeout des requ√™tes r√©seau (10s)
- Timeout JavaScript rendering (30s)

---

## üöÄ √âvolutions Futures

- [ ] Job asynchrone pour le t√©l√©chargement d'images (via Solid Queue)
- [ ] Support de proxies pour √©viter les bans
- [ ] D√©tection automatique de captchas
- [ ] Historique des modifications de prix
- [ ] Webhooks pour surveiller les mises √† jour d'annonces
- [ ] API tierce (ScrapingBee, BrightData) en fallback

---

## üìö Ressources

- [Geocoder Gem](https://github.com/alexreisner/geocoder)
- [Down Gem](https://github.com/janko/down)
- [Ferrum Gem](https://github.com/rubycdp/ferrum)
- [Nominatim Usage Policy](https://operations.osmfoundation.org/policies/nominatim/)
- [Active Storage Guide](https://guides.rubyonrails.org/active_storage_overview.html)

---

**Version** : 2.0  
**Date** : 24 f√©vrier 2026  
**Auteur** : GitHub Copilot

